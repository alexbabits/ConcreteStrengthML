{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission File:\n",
    "\n",
    "For each id in the test set, you must predict the value for the target Strength. \n",
    "The file should contain a header and have the following format:\n",
    "\n",
    "id, Strength\n",
    "\n",
    "5439, 55.2\n",
    "\n",
    "5440, 12.3\n",
    "\n",
    "5441, 83.4\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: xgboost in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.7.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from xgboost) (1.24.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from xgboost) (1.10.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# installs and imports here (Sklearn, matplotlib, numpy, pandas, etc.)\n",
    "%pip install scikit-learn \n",
    "%pip install pandas\n",
    "%pip install xgboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CementComponent</th>\n",
       "      <th>BlastFurnaceSlag</th>\n",
       "      <th>FlyAshComponent</th>\n",
       "      <th>WaterComponent</th>\n",
       "      <th>SuperplasticizerComponent</th>\n",
       "      <th>CoarseAggregateComponent</th>\n",
       "      <th>FineAggregateComponent</th>\n",
       "      <th>AgeInDays</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>525.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>613.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>143.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>967.0</td>\n",
       "      <td>643.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>289.0</td>\n",
       "      <td>134.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>795.3</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>304.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>157.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>935.4</td>\n",
       "      <td>781.2</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5402</th>\n",
       "      <td>446.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>967.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5403</th>\n",
       "      <td>350.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>974.0</td>\n",
       "      <td>775.0</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5404</th>\n",
       "      <td>295.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1076.2</td>\n",
       "      <td>759.3</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5405</th>\n",
       "      <td>376.0</td>\n",
       "      <td>93.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.6</td>\n",
       "      <td>11.5</td>\n",
       "      <td>955.8</td>\n",
       "      <td>662.9</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5406</th>\n",
       "      <td>190.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125.2</td>\n",
       "      <td>166.6</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1079.0</td>\n",
       "      <td>770.1</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5407 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CementComponent  BlastFurnaceSlag  FlyAshComponent  WaterComponent  \\\n",
       "id                                                                         \n",
       "0               525.0               0.0              0.0           186.0   \n",
       "1               143.0             169.0            143.0           191.0   \n",
       "2               289.0             134.7              0.0           185.7   \n",
       "3               304.0              76.0              0.0           228.0   \n",
       "4               157.0             236.0              0.0           192.0   \n",
       "...               ...               ...              ...             ...   \n",
       "5402            446.0              24.0             79.0           162.0   \n",
       "5403            350.0               0.0              0.0           203.0   \n",
       "5404            295.8               0.0              0.0           185.7   \n",
       "5405            376.0              93.4              0.0           162.6   \n",
       "5406            190.7               0.0            125.2           166.6   \n",
       "\n",
       "      SuperplasticizerComponent  CoarseAggregateComponent  \\\n",
       "id                                                          \n",
       "0                           0.0                    1125.0   \n",
       "1                           8.0                     967.0   \n",
       "2                           0.0                    1075.0   \n",
       "3                           0.0                     932.0   \n",
       "4                           0.0                     935.4   \n",
       "...                         ...                       ...   \n",
       "5402                       11.6                     967.0   \n",
       "5403                        0.0                     974.0   \n",
       "5404                        0.0                    1076.2   \n",
       "5405                       11.5                     955.8   \n",
       "5406                        7.9                    1079.0   \n",
       "\n",
       "      FineAggregateComponent  AgeInDays  \n",
       "id                                       \n",
       "0                      613.0          3  \n",
       "1                      643.0         28  \n",
       "2                      795.3         28  \n",
       "3                      670.0        365  \n",
       "4                      781.2         90  \n",
       "...                      ...        ...  \n",
       "5402                   712.0          3  \n",
       "5403                   775.0        180  \n",
       "5404                   759.3         28  \n",
       "5405                   662.9         28  \n",
       "5406                   770.1         56  \n",
       "\n",
       "[5407 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preparation\n",
    "\n",
    "# Put all the data we are going to use into a df. Puts target variable column into 'y'. 'X' is all data except 'y' column. Splits the data.\n",
    "data_df = pd.read_csv('./data.csv', index_col='id')\n",
    "y = data_df.Strength\n",
    "X = data_df.drop('Strength', axis=1)\n",
    "# Convert columns with mix of 0's and numbers into  binary, to see the effects. Actually made it a bit better.\n",
    "#binary_columns = ['BlastFurnaceSlag', 'FlyAshComponent', 'SuperplasticizerComponent']\n",
    "#for col in binary_columns:\n",
    "    #X[col] = X[col].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to find the best parameters for the random forest model while avoiding overfitting.\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 400],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'min_samples_leaf': [6, 8, 10]\n",
    "}\n",
    "\n",
    "# Create a RandomForestRegressor model instance\n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "# Create the GridSearchCV instance\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, scoring='neg_root_mean_squared_error', cv=5, n_jobs=4)\n",
    "\n",
    "# Fit the GridSearchCV instance to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding mean RMSE\n",
    "best_params = grid_search.best_params_\n",
    "best_score = -grid_search.best_score_\n",
    "print(f\"Best hyperparameters for the Random Forest model: {best_params}\")\n",
    "print(f\"Mean RMSE for the Random Forest model with the best hyperparameters: {best_score}\")\n",
    "\n",
    "# {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200} yields RMSE 12.26\n",
    "# {'max_depth': 5, 'min_samples_leaf': 8, 'min_samples_split': 15, 'n_estimators': 100} yields RMSE 12.16\n",
    "# {'max_depth': 7, 'min_samples_leaf': 8, 'min_samples_split': 10, 'n_estimators': 200} yields RMSE 12.154 \n",
    "# {'max_depth': 6, 'min_samples_leaf': 8, 'min_samples_split': 10, 'n_estimators': 200} yields RMSE 12.14688\n",
    "# {'max_depth': 6, 'min_samples_leaf': 8, 'min_samples_split': 9, 'n_estimators': 200} yields RMSE 12.150\n",
    "# {'max_depth': 6, 'min_samples_leaf': 8, 'min_samples_split': 10, 'n_estimators': 200} yields RMSE 12.14188\n",
    "# {'max_depth': 6, 'min_samples_leaf': 8, 'min_samples_split': 10, 'n_estimators': 400} yields RMSE 12.1503\n",
    "# {'max_depth': 4, 'min_samples_leaf': 8, 'min_samples_split': 10, 'n_estimators': 400} yields RMSE 12.2416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for the XGBoost model: {'colsample_bytree': 0.4, 'gamma': 0.1, 'learning_rate': 0.005, 'max_depth': 4, 'n_estimators': 1600, 'subsample': 0.7}\n",
      "Mean RMSE for the XGBoost model with the best hyperparameters: 12.127089570489332\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter search space for XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [1600],\n",
    "    'max_depth': [4],\n",
    "    'learning_rate': [0.005],\n",
    "    'subsample': [0.7],\n",
    "    'colsample_bytree': [0.4],\n",
    "    'gamma': [0.1]\n",
    "}\n",
    "\n",
    "# Create an XGBRegressor model instance\n",
    "xgb_model = XGBRegressor()\n",
    "\n",
    "# Create the GridSearchCV instance\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='neg_root_mean_squared_error', cv=5, n_jobs=4)\n",
    "\n",
    "# Fit the GridSearchCV instance to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding mean RMSE\n",
    "best_params = grid_search.best_params_\n",
    "best_score = -grid_search.best_score_\n",
    "print(f\"Best hyperparameters for the XGBoost model: {best_params}\")\n",
    "print(f\"Mean RMSE for the XGBoost model with the best hyperparameters: {best_score}\")\n",
    "\n",
    "# {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 200, 'subsample': 1} yields RMSE 12.258\n",
    "# {'colsample_bytree': 0.35, 'gamma': 0, 'learning_rate': 0.025, 'max_depth': 6, 'n_estimators': 400, 'subsample': 0.75} yields RMSE 12.28\n",
    "# {'colsample_bytree': 0.42, 'gamma': 0, 'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 800, 'subsample': 0.75} yields RMSE 12.2175\n",
    "# {'colsample_bytree': 0.38, 'gamma': 0.05, 'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 800, 'subsample': 0.68} yields RMSE 12.2167\n",
    "# {'colsample_bytree': 0.4, 'gamma': 0.1, 'learning_rate': 0.005, 'max_depth': 6, 'n_estimators': 800, 'subsample': 0.7} yields RMSE 12.365\n",
    "# {'colsample_bytree': 0.4, 'gamma': 0.1, 'learning_rate': 0.005, 'max_depth': 5, 'n_estimators': 1600, 'subsample': 0.7} yields RMSE 12.1586\n",
    "# {'colsample_bytree': 0.4, 'gamma': 0.1, 'learning_rate': 0.005, 'max_depth': 4, 'n_estimators': 1600, 'subsample': 0.7} yields RMSE 12.127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instances of linear regression, random forrest, and XGBoost.\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=400, max_depth=5, min_samples_split=10, min_samples_leaf=8),\n",
    "    'XGBoost': XGBRegressor(n_estimators=1600, max_depth=4, learning_rate=0.005, subsample=0.7, colsample_bytree=0.4, gamma=0.1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE for the Linear Regression model with 5-fold cross-validation: 14.608373730656874\n",
      "RMSE for the Linear Regression model by splitting the data: 14.269586411578604\n",
      "\n",
      "Mean RMSE for the Random Forest model with 5-fold cross-validation: 12.171904102613812\n",
      "RMSE for the Random Forest model by splitting the data: 11.900186991628125\n",
      "\n",
      "Mean RMSE for the XGBoost model with 5-fold cross-validation: 12.127089570489332\n",
      "RMSE for the XGBoost model by splitting the data: 11.838597343166393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through the models and perform cross-validation and train/test split evaluations\n",
    "for model_name, model in models.items():\n",
    "    # Perform 5-fold cross-validation, calculate mean rmse\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "    mean_rmse = -np.mean(cv_scores)\n",
    "    print(f\"Mean RMSE for the {model_name} model with 5-fold cross-validation: {mean_rmse}\")\n",
    "\n",
    "    # Train the model with the training split data, make predictions, calculate rmse\n",
    "    train_X, validation_X, train_y, validation_y = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    validation_predictions = model.predict(validation_X)\n",
    "    rmse = mean_squared_error(validation_y, validation_predictions, squared=False)\n",
    "    print(f\"RMSE for the {model_name} model by splitting the data: {rmse}\")\n",
    "    print()  # Add a newline for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all the data we are going to use into a df.\n",
    "test_df = pd.read_csv('./test.csv', index_col='id')\n",
    "X_test = test_df\n",
    "# Convert columns with mix of 0's and numbers into binary, to see the effects. Actually made it a bit better.\n",
    "#binary_columns = ['BlastFurnaceSlag', 'FlyAshComponent', 'SuperplasticizerComponent']\n",
    "#for col in binary_columns:\n",
    "    #X_test[col] = X_test[col].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model = XGBRegressor(n_estimators=1600, max_depth=4, learning_rate=0.005, subsample=0.7, colsample_bytree=0.4, gamma=0.1)\n",
    "xgboost_model.fit(X, y)\n",
    "test_y_pred = xgboost_model.predict(X_test)\n",
    "\n",
    "# Create a DataFrame with the test IDs and predictions\n",
    "submission_df = pd.DataFrame({'id': X_test.index, 'Strength': test_y_pred})\n",
    "# Save the DataFrame to a CSV file for submitting to Kaggle\n",
    "submission_df.to_csv('submissionXGBnobinary.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ended up getting these scores which would have placed me in the top 53 out of 765 (For my XGB model with binary columns):\n",
    "\n",
    "XGB: 12.27347\n",
    "XGB no binary: 12.29858\n",
    "RF: 12.3461\n",
    "RF no binary: 12.32284"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
